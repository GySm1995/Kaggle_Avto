# Kaggle_Avto
Решение задачи регрессии - предсказание цены автомобиля на вторичном рынке
Описание задачи
Многие знают про маркетплейсы где продаются б/у вещи, на которых есть возможность недорого купить качественную и полезную вещь. Но всегда волнует вопрос - кто и как устанавливает цену, и какие его характеристики больше всего влияют на итоговую стоимость продажи?! Вопрос становиться особо актуальным, если речь идет про дорогие товары, например про автомобили!
Предлагаем вам принять участие в Мастерской, в рамках которой вы сможете поработать с данными о продажах автомобилей на вторичном рынке. Целью вашего проекта будет разработанная модель предсказания стоимости автомобиля на вторичном рынке.

В данном проекте представлено решение соревнования на платформе Kaggle в рамках обучения в ЯндексПрактикуме - модуль "Мастерская 1".

В рамках работы реализовано следующие:

    1. Проведен анализ данных для задачи регрессии - прогнозирование цены автомобиля на вторичном рынке.
    2. Заполнены пропуски в данных и исключены выбросы. Пропуски заполнялись следующим образом. Отсутствие пробега - данные исключались, отсутствующие категориальные данные - вводилась категория "не указано" (Н/у).
    3. Для обучения моделей категориальные данные кодировались методом OHE (количество категорий уменьшалось путем удаления повторений), за исключением данных по производителю, продовцу и типу кузова - здесь данные кодировались так: категории присваивалось число равное частоте встречаемости в данных. Количественные данные нормировались методом MinMaxScaller. Общее количество признаков для обучения составило - 151, количество данных ~ 410к.
    4. Было обучено и протестировано 8 алгоритмов обучения. 4 алгоритма на основе ансамбля: Random Forest Regressor, Gradient Boosting Regressor , LightGBM , XGBoost  (CatBoost-не разобрался с ошибкой). Линейной моделью была Linear Regressor. Также были модели обченные методами MLPRegressor и DecisionTreeRegressor. Все модели были обучены с некоторыми выбранными гиперпараметрами, с целью уменьшения переобучения без больших вычислительных затрат. Все модели оценивались с использованием показателя MAPE - средняя абсолютная ошибка в процентах. Для каждой модели был создан график, показывающий результирующий прогноз по сравнению с метками в виде подгонки модели. Некоторые из моделей имели возможность учитывать категориальные признаки и кодировать их напрямую - регрессор LightGBM и CatBoost (не разобрадся с ошибкой). Были созданы две модели LightGBM, одна из которых не учитывала категориальные признаки и принимала основные предварительно обработанные данные, а другая принимала необработанные данные с категориальными признаками, имеющими тип данных «категория». Оба разделения данных были сгенерированы/разделены с использованием одного и того же случайного состояния, чтобы можно было воспроизвести результаты. Остальные модели были обучены на данных, которые предварительно нормировались и кодировались. 
    5. За неимением достаточного количества времени не имелось возможность произвести перебор гиперпараметров для лучших моделей по большей сетке, чем предварительная.
    6. Наилучший показатель MAPE на тестовых данныхпоказала модель LightGBM, обученная на размеченных данных "в ручную" методом LightGBM (с гиперпараметрами 'regressor__learning_rate': 0.05, 'regressor__n_estimators': 100, 'regressor__num_leaves': 120, 'regressor__random_state': 1) MAPE, которой равен 19.58 (19.41).
    7. Оценка важности признаков для лучшей модели показала, что наиболее значимые признаки (в порядке убывания значимости) - модель, модификация, год выпуска, пробег и тип кузова.
    8. Предполагается доработка задачи с поиском оптимальных параметров на большей сетке, при кросс-валидации данные не перемешивались, что необходимо исправить. 
